import re
import os
import glob
import pandas as pd
from datetime import date
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.hooks.postgres_hook import PostgresHook



def copy_to_local_from_s3bucket(bucket_name: str, key: str, local_path: str) -> None:
    """
    Downloads a file from an S3 bucket and copies it to a local directory.

    Args
        bucket_name (str): The name of the S3 bucket.
        key (str): The key of the file to be downloaded.
        local_path (str): The local directory to copy the file to.

    Returns: None if the download and copy are successful, or a string error message if not.
    """
    s3 = S3Hook()
    try:       
        s3.download_file(
            bucket_name=bucket_name, 
            key=key, 
            local_path=local_path, 
            preserve_file_name=True, 
            use_autogenerated_subdir=False)        
    except Exception as e:
        print('\n' + '*'*99 + f'\n{e}\n'+ '*'*99)

def transform_data_to_silver() -> None:
    """
    Transform the bronze layer data from parquet to csv format, 
    apply data cleaning, and insert data into the silver layer table.

    Raises:
        FileNotFoundError: If the bronze layer file does not exist.
    """
    try:   
        df_bronze = pd.read_parquet(f'./data/randomuser/{str(date.today())}_randomuser.parquet')             
        print('\n' + '*'*99 + f'\n{df_bronze.shape}\n'+ '*'*99) 
    except Exception as e:
        print('\n' + '*'*99 + f'\n{e}\n'+ '*'*99)

    df_bronze['address'] = df_bronze['location.street.name'] + ', ' + df_bronze['location.street.number']
    del_columns = ['location.street.number', 'location.street.name', 'location.postcode', 'location.timezone.description', 'location.timezone.offset', 'login.uuid', \
                                'login.username', 'login.password', 'login.salt', 'login.md5', 'login.sha1', 'login.sha256', 'id.name', 'id.value', 'picture.medium', 'picture.thumbnail']
    df_bronze = df_bronze.drop(del_columns,  axis=1)
    rename_columns = ['gender', 'email', 'phone', 'cellphone', 'nationality', 'title_name', 'first_name', 'last_name', 'city', 'state', 'country', 'latitude', 'longitude', \
                                    'dob_date', 'age', 'register_date', 'register_age', 'photo', 'address']
    colunas_ordenar = ['title_name', 'first_name', 'last_name', 'gender', 'age', 'dob_date', 'nationality', 'photo','email', 'phone', 'cellphone', 'address', 'city', 'state', \
                                 'country', 'latitude', 'longitude', 'register_date', 'register_age']
    df_bronze.columns = rename_columns    
    df_bronze = df_bronze[colunas_ordenar]
    
    df_bronze['not_arabic'] = df_bronze['first_name'].apply(lambda s: bool(re.compile(r'^[a-zA-ZáéíóúÁÉÍÓÚ]+$').match(str(s))))
    df_bronze = df_bronze[df_bronze['not_arabic'] == True]    
    df_bronze.pop(df_bronze.columns[-1])

    if os.path.isfile('./data/randomuser/table_silver.csv'):
        df_silver = pd.read_csv('./data/randomuser/table_silver.csv')
        df_silver.pop(df_silver.columns[0])
        df_final = pd.concat([df_bronze, df_silver])
        df_final.to_csv('./data/randomuser/table_silver.csv')
        print('\n' + '*'*99 + '\nNew data inserted into the table successfully.\n'+ '*'*99)    
    else:
        df_bronze.to_csv('./data/randomuser/table_silver.csv')
        print('\n' + '*'*99 + '\nCreated Table in Silver Layer Successfully.\n'+ '*'*99)

def local_to_s3(filename: str, bucket_name: str, key: str, filepath: str, flag: bool = True) -> None:
    """
    Loads a file from local to an S3 bucket and deletes local files if flag is True.
    
    Args:
        filename (str): The name of the file to be uploaded.
        bucket_name (str): The name of the S3 bucket to upload the file to.
        key (str): The key name of the object in the S3 bucket.
        filepath (str): The path to the local file(s) to be deleted.
        flag (bool, optional): If True, deletes the local file(s) after uploading. Defaults to True.
    """
    s3 = S3Hook()
    try:
        s3.load_file(
        filename=filename,
        bucket_name=bucket_name,
        replace=True,
        key=key
        )
        print('\n' + '*'*99 + f'\nInseriu o BR\n'+ '*'*99)

        if flag:
            files=glob.glob(filepath)
            if not files:
                raise FileNotFoundError(f'Could not find any files at {filepath}.')
            for f in files:
                os.remove(f)

    except Exception as e:
        print('\n' + '*'*99 + f'\n{e}\n'+ '*'*99)

def transform_csv_to_gold() -> None:
    """
    Transforms a CSV table from the Silver layer into a table in the Gold layer. The table is divided by nationality.
    The table for the Brazilian nationality is generated first.
    """

    df_silver = pd.read_csv('./data/randomuser/table_silver.csv')
    df_silver.drop('Unnamed: 0', axis=1, inplace=True)
    listNationalities = ['BR', 'CH', 'FR']
    for nat in listNationalities:
         globals()[f'df_gold_{nat}_temp'] = df_silver[df_silver.nationality == nat]

    print('\n' + '*'*99 + f'\n{df_gold_BR_temp.shape}\n'+ '*'*99)

    if os.path.isfile('./data/randomuser/table_gold_BR.csv'):
        df_gold_BR = pd.read_csv('./data/randomuser/table_gold_BR.csv')
        df_gold_BR.pop(df_gold_BR.columns[0])
        df_gold_BR = pd.concat([df_gold_BR_temp, df_gold_BR])
        df_gold_BR.to_csv('./data/randomuser/table_gold_BR.csv')

        print('\n' + '*'*99 + '\nEfetuada a inserção de novos dados na tabela.\n'+ '*'*99)    
    else:
        df_gold_BR_temp.to_csv('./data/randomuser/table_gold_BR.csv')        
        print('\n' + '*'*99 + '\nCriada Tabela BR na Camada Gold com Sucesso\n'+ '*'*99)


def populate_table() -> None:
    """
    Populates the PostgreSQL table tb_random_users with data from a CSV file.

    Raises:
        FileNotFoundError: if no files are found at the specified path.

    """
    pg_hook = PostgresHook(postgres_conn_id='postgres-airflow')
    df = pd.read_csv('./data/randomuser/table_gold_BR.csv')
    df.drop('Unnamed: 0', axis=1, inplace=True)

    for _, row in df.iterrows():       
        try:
            pg_hook.run(f"INSERT INTO tb_random_users (title_name, first_name, last_name, gender, age, dob_date, nationality, photo, email, phone, cellphone, address, city, state, country, \
                        latitude, longitude, register_date, register_age, insertion_db_date) VALUES ('{str(row.title_name)}', '{str(row.first_name)}', '{str(row.last_name)}', '{str(row.gender)}', '{str(row.age)}', \
                        '{str(row.dob_date)}', '{str(row.nationality)}', '{str(row.photo)}', '{str(row.email)}', '{str(row.phone)}', '{str(row.cellphone)}', '{str(row.address)}', '{str(row.city)}',\
                         '{str(row.state)}', '{str(row.country)}', '{str(row.latitude)}', '{str(row.longitude)}', '{str(row.register_date)}', '{str(row.register_age)}', now())")
            print('*'*100 + f'\nRegistro de {row.first_name} {row.last_name} inserido com sucesso na base de dados.\n' + '*'*100)
        except Exception as e:
            print(f'Erro: {e}')
    files=glob.glob('./data/randomuser/*')
    if not files:
        raise FileNotFoundError(f'Could not find any files at ./data/randomuser/.')
    for f in files:
        os.remove(f)